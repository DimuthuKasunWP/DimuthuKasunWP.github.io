<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How To Train an Object Detection Classifier for Multiple Objects Using TensorFlow (GPU) on Windows…</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How To Train an Object Detection Classifier for Multiple Objects Using TensorFlow (GPU) on Windows…</h1>
</header>
<section data-field="subtitle" class="p-summary">
This is for how to use TensorFlow’s Object Detection API to train an object detection classifier for multiple objects on Windows 10, 8, or…
</section>
<section data-field="body" class="e-content">
<section name="f69c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b58c" id="b58c" class="graf graf--h3 graf--leading graf--title">How To Train an Object Detection Classifier for Multiple Objects Using TensorFlow (GPU) on Windows 10</h3><p name="fe55" id="fe55" class="graf graf--p graf-after--h3">How to use TensorFlow’s Object Detection API to train an object detection classifier for multiple objects on Windows 10, 8, or 7. (It will also work on Linux-based OSes with some minor changes.) It was originally written using TensorFlow version 1.5, but will also work for newer versions of TensorFlow.</p><p name="13c3" id="13c3" class="graf graf--p graf-after--p">This article describes every step required to get going with your own object detection classifier:</p><ol class="postList"><li name="3445" id="3445" class="graf graf--li graf-after--p">Installing Anaconda, CUDA, and cuDNN</li><li name="ec3b" id="ec3b" class="graf graf--li graf-after--li">Setting up the Object Detection directory structure and Anaconda Virtual Environment</li><li name="e463" id="e463" class="graf graf--li graf-after--li">Gathering and labeling pictures</li><li name="7087" id="7087" class="graf graf--li graf-after--li">Generating training data</li><li name="fe73" id="fe73" class="graf graf--li graf-after--li">Creating a label map and configuring training</li><li name="cc41" id="cc41" class="graf graf--li graf-after--li">Training</li><li name="cee4" id="cee4" class="graf graf--li graf-after--li">Exporting the inference graph</li><li name="302b" id="302b" class="graf graf--li graf-after--li">Testing and using your newly trained object detection classifier</li></ol><h3 name="4e66" id="4e66" class="graf graf--h3 graf-after--li">Introduction</h3><p name="0639" id="0639" class="graf graf--p graf-after--h3">The purpose of this article is to explain how to train your own convolutional neural network object detection classifier for multiple objects, starting from scratch. At the end of this article, you will have a program that can identify and draw boxes around specific objects in pictures, videos, or in a webcam feed.</p><p name="16c1" id="16c1" class="graf graf--p graf-after--p">There are several good tutorials available for how to use TensorFlow’s Object Detection API to train a classifier for a single object. However, these usually assume you are using a Linux operating system. If you’re like me, you might be a little hesitant to install Linux on your high-powered gaming PC that has the sweet graphics card you’re using to train a classifier. The Object Detection API seems to have been developed on a Linux-based OS. To set up TensorFlow to train a model on Windows, there are several workarounds that need to be used in place of commands that would work fine on Linux. Also, this article provides instructions for training a classifier that can detect multiple objects, not just one.</p><p name="ec99" id="ec99" class="graf graf--p graf-after--p">The article is written for Windows 10, and it will also work for Windows 7 and 8. The general procedure can also be used for Linux operating systems, but file paths and package installation commands will need to change accordingly. I used TensorFlow-GPU v1.5 while writing the initial version of this article, but it will likely work for future versions of TensorFlow.</p><p name="85b5" id="85b5" class="graf graf--p graf-after--p">TensorFlow-GPU allows your PC to use the video card to provide extra processing power while training, so it will be used for this article. In my experience, using TensorFlow-GPU instead of regular TensorFlow reduces training time by a factor of about 8 (3 hours to train instead of 24 hours). The CPU-only version of TensorFlow can also be used for this article, but it will take longer. If you use CPU-only TensorFlow, you do not need to install CUDA and cuDNN in Step 1.</p><h3 name="5edb" id="5edb" class="graf graf--h3 graf-after--p">Steps</h3><h4 name="ee16" id="ee16" class="graf graf--h4 graf-after--h3">1. Install Anaconda, CUDA, and cuDNN</h4><p name="02bf" id="02bf" class="graf graf--p graf-after--h4">Follow <a href="https://www.youtube.com/watch?v=RplXYjxgZbw" data-href="https://www.youtube.com/watch?v=RplXYjxgZbw" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this YouTube video by Mark Jay</a>, which shows the process for installing Anaconda, CUDA, and cuDNN. You do not need to actually install TensorFlow as shown in the video, because we will do that later in Step 2. The video is made for TensorFlow-GPU v1.4, so download and install the CUDA and cuDNN versions for the latest TensorFlow version, rather than CUDA v8.0 and cuDNN v6.0 as instructed in the video. The <a href="https://www.tensorflow.org/install/gpu" data-href="https://www.tensorflow.org/install/gpu" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TensorFlow website</a> indicates which versions of CUDA and cuDNN are needed for the latest version of TensorFlow.</p><p name="ac81" id="ac81" class="graf graf--p graf-after--p">If you are using an older version of TensorFlow, make sure you use the CUDA and cuDNN versions that are compatible with the TensorFlow version you are using. <a href="https://www.tensorflow.org/install/source#tested_build_configurations" data-href="https://www.tensorflow.org/install/source#tested_build_configurations" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Here</a> is a table showing which version of TensorFlow requires which versions of CUDA and cuDNN.</p><p name="aa5c" id="aa5c" class="graf graf--p graf-after--p">Be sure to install <a href="https://www.anaconda.com/distribution/#download-section" data-href="https://www.anaconda.com/distribution/#download-section" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Anaconda</a> as instructed in the video, because the Anaconda virtual environment will be used for the rest of this article. (Note: The current version of Anaconda uses Python 3.7, which is not officially supported by TensorFlow. However, when creating an Anaconda virtual environment during Step 2d of this article, we will tell it to use Python 3.5.)</p><p name="80a0" id="80a0" class="graf graf--p graf-after--p">Visit <a href="https://www.tensorflow.org/install" data-href="https://www.tensorflow.org/install" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">TensorFlow’s website</a> for further installation details, including how to install it on other operating systems (like Linux). The <a href="https://github.com/tensorflow/models/tree/master/research/object_detection" data-href="https://github.com/tensorflow/models/tree/master/research/object_detection" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">object detection repository</a> itself also has <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">installation instructions</a>.</p><h4 name="8a1f" id="8a1f" class="graf graf--h4 graf-after--p">2. Set up TensorFlow Directory and Anaconda Virtual Environment</h4><p name="5d0f" id="5d0f" class="graf graf--p graf-after--h4">The TensorFlow Object Detection API requires using the specific directory structure provided in its GitHub repository. It also requires several additional Python packages, specific additions to the PATH and PYTHONPATH variables, and a few extra setup commands to get everything set up to run or train an object detection model.</p><p name="8c14" id="8c14" class="graf graf--p graf-after--p">This portion of the article goes over the full set up required. It is fairly meticulous, but follow the instructions closely, because improper setup can cause unwieldy errors down the road.</p><p name="be9e" id="be9e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2a. Download TensorFlow Object Detection API repository from GitHub</strong></p><p name="ae49" id="ae49" class="graf graf--p graf-after--p">Create a folder directly in C: and name it “tensorflow1”. This working directory will contain the full TensorFlow object detection framework, as well as your training images, training data, trained classifier, configuration files, and everything else needed for the object detection classifier.</p><p name="1dca" id="1dca" class="graf graf--p graf-after--p">Download the full TensorFlow object detection repository located at <a href="https://github.com/tensorflow/models" data-href="https://github.com/tensorflow/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/tensorflow/models</a> by clicking the “Clone or Download” button and downloading the zip file. Open the downloaded zip file and extract the “models-master” folder directly into the C:\tensorflow1 directory you just created. Rename “models-master” to just “models”.</p><p name="7f33" id="7f33" class="graf graf--p graf-after--p">Note: The TensorFlow model&#39;s repository’s code (which contains the object detection API) is continuously updated by the developers. Sometimes they make changes that break functionality with old versions of TensorFlow. It is always best to use the latest version of TensorFlow and download the latest models repository. If you are not using the latest version, clone or download the commit for the version you are using as listed in the table below.</p><p name="dd24" id="dd24" class="graf graf--p graf-after--p">If you are using an older version of TensorFlow, here is a table showing which GitHub commit of the repository you should use. I generated this by going to the release branches for the model&#39;s repository and getting the commit before the last commit for the branch. (They remove the research folder as the last commit before they create the official version release.)</p><p name="0002" id="0002" class="graf graf--p graf-after--p">This is originally done using TensorFlow v1.5 and this <a href="https://github.com/tensorflow/models" data-href="https://github.com/tensorflow/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub commit</a> of the TensorFlow Object Detection API. If portions of this article do not work, it may be necessary to install TensorFlow v1.5 and use this exact commit rather than the most up-to-date version.</p><p name="e6e8" id="e6e8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2b. Download the Faster-RCNN-Inception-V2-COCO model from TensorFlow’s model zoo</strong></p><p name="f031" id="f031" class="graf graf--p graf-after--p">TensorFlow provides several object detection models (pre-trained classifiers with specific neural network architectures) in its <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">model zoo</a>. Some models (such as the SSD-MobileNet model) have an architecture that allows for faster detection but with less accuracy, while some models (such as the Faster-RCNN model) give slower detection but with more accuracy. I initially started with the SSD-MobileNet-V1 model, but it didn’t do a very good job identifying the cards in my images. I re-trained my detector on the Faster-RCNN-Inception-V2 model, and the detection worked considerably better, but with a noticeably slower speed.</p><figure name="cfb2" id="cfb2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 286px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.9%;"></div><img class="graf-image" data-image-id="1*jVkDYf-tMHdSpfQ7rgbFhA.jpeg" data-width="1006" data-height="411" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*jVkDYf-tMHdSpfQ7rgbFhA.jpeg"></div></figure><p name="2cc3" id="2cc3" class="graf graf--p graf-after--figure">You can choose which model to train your objection detection classifier on. If you are planning on using the object detector on a device with low computational power (such as a smart phone or Raspberry Pi), use the SDD-MobileNet model. If you will be running your detector on a decently powered laptop or desktop PC, use one of the RCNN models.</p><p name="c639" id="c639" class="graf graf--p graf-after--p">This article will use the Faster-RCNN-Inception-V2 model. <a href="http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz" data-href="http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Download the model here.</a> Open the downloaded faster_rcnn_inception_v2_coco_2018_01_28.tar.gz file with a file archiver such as WinZip or 7-Zip and extract the faster_rcnn_inception_v2_coco_2018_01_28 folder to the C:\tensorflow1\models\research\object_detection folder. (Note: The model date and version will likely change in the future, but it should still work with this article .)</p><p name="5f97" id="5f97" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2c. Download this article’s repository from GitHub</strong></p><p name="9065" id="9065" class="graf graf--p graf-after--p">Download the full repository located on <a href="https://github.com/DimuthuKasunWP/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git" data-href="https://github.com/DimuthuKasunWP/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this page </a>and extract all the contents directly into the C:\tensorflow1\models\research\object_detection directory. (You can overwrite the existing “README.md” file.) This establishes a specific directory structure that will be used for the rest of the article.</p><p name="2ffa" id="2ffa" class="graf graf--p graf-after--p">At this point, here is what your \object_detection folder should look like:</p><figure name="8202" id="8202" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 612px; max-height: 510px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 83.3%;"></div><img class="graf-image" data-image-id="1*V9kOu2YJYI71VAJQlgz_0g.jpeg" data-width="612" data-height="510" src="https://cdn-images-1.medium.com/max/800/1*V9kOu2YJYI71VAJQlgz_0g.jpeg"></div></figure><p name="0be0" id="0be0" class="graf graf--p graf-after--figure">This repository contains the images, annotation data, .csv files, and TFRecords needed to train a “Pinochle Deck” playing card detector. You can use these images and data to practice making your own Pinochle Card Detector. It also contains Python scripts that are used to generate the training data. It has scripts to test out the object detection classifier on images, videos, or a webcam feed. You can ignore the \doc folder and its files; they are just there to hold the images used for this readme.</p><p name="8e67" id="8e67" class="graf graf--p graf-after--p">If you want to practice training your own “Pinochle Deck” card detector, you can leave all the files as they are. You can follow along with this article to see how each of the files were generated, and then run the training. You will still need to generate the TFRecord files (train.record and test.record) as described in Step 4.</p><p name="8eaa" id="8eaa" class="graf graf--p graf-after--p">You can also download the frozen inference graph for my trained Pinochle Deck card detector <a href="https://www.dropbox.com/s/va9ob6wcucusse1/inference_graph.zip?dl=0" data-href="https://www.dropbox.com/s/va9ob6wcucusse1/inference_graph.zip?dl=0" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">from this Dropbox link</a> and extract the contents to \object_detection\inference_graph. This inference graph will work “out of the box”. You can test it after all the setup instructions in Step 2a — 2f have been completed by running the Object_detection_image.py (or video or webcam) script.</p><p name="8d5b" id="8d5b" class="graf graf--p graf-after--p">If you want to train your own object detector, delete the following files (do not delete the folders):</p><ul class="postList"><li name="c272" id="c272" class="graf graf--li graf-after--p">All files in \object_detection\images\train and \object_detection\images\test</li><li name="ef0e" id="ef0e" class="graf graf--li graf-after--li">The “test_labels.csv” and “train_labels.csv” files in \object_detection\images</li><li name="7ad1" id="7ad1" class="graf graf--li graf-after--li">All files in \object_detection\training</li><li name="2e0a" id="2e0a" class="graf graf--li graf-after--li">All files in \object_detection\inference_graph</li></ul><p name="26d9" id="26d9" class="graf graf--p graf-after--li">Now, you are ready to start from scratch in training your own object detector. This article will assume that all the files listed above were deleted, and will go on to explain how to generate the files for your own training dataset.</p><p name="27e0" id="27e0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2d. Set up new Anaconda virtual environment</strong></p><p name="7ac4" id="7ac4" class="graf graf--p graf-after--p">Next, we’ll work on setting up a virtual environment in Anaconda for tensorflow-gpu. From the Start menu in Windows, search for the Anaconda Prompt utility, right click on it, and click “Run as Administrator”. If Windows asks you if you would like to allow it to make changes to your computer, click Yes.</p><p name="0eff" id="0eff" class="graf graf--p graf-after--p">In the command terminal that pops up, create a new virtual environment called “tensorflow1” by issuing the following command:</p><pre name="b66c" id="b66c" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">C:\&gt; conda create -n tensorflow1 pip python=3.5</code></pre><p name="503d" id="503d" class="graf graf--p graf-after--pre">Then, activate the environment and update pip by issuing:</p><pre name="fe03" id="fe03" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">C:\&gt; activate tensorflow1</code></pre><pre name="5d00" id="5d00" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">(tensorflow1) C:\&gt;python -m pip install --upgrade pip</code></pre><p name="b878" id="b878" class="graf graf--p graf-after--pre">Install tensorflow-gpu in this environment by issuing:</p><pre name="ea6d" id="ea6d" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\&gt; pip install --ignore-installed --upgrade tensorflow-gpu</code></pre><p name="0ce6" id="0ce6" class="graf graf--p graf-after--pre">(Note: You can also use the CPU-only version of TensorFow, but it will run much slower. If you want to use the CPU-only version, just use “tensorflow” instead of “tensorflow-gpu” in the previous command.)</p><p name="f70a" id="f70a" class="graf graf--p graf-after--p">Install the other necessary packages by issuing the following commands:</p><pre name="6620" id="6620" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\&gt; conda install -c anaconda protobuf<br>(tensorflow1) C:\&gt; pip install pillow<br>(tensorflow1) C:\&gt; pip install lxml<br>(tensorflow1) C:\&gt; pip install Cython<br>(tensorflow1) C:\&gt; pip install contextlib2<br>(tensorflow1) C:\&gt; pip install jupyter<br>(tensorflow1) C:\&gt; pip install matplotlib<br>(tensorflow1) C:\&gt; pip install pandas<br>(tensorflow1) C:\&gt; pip install opencv-python</code></pre><p name="5073" id="5073" class="graf graf--p graf-after--pre">(Note: The ‘pandas’ and ‘opencv-python’ packages are not needed by TensorFlow, but they are used in the Python scripts to generate TFRecords and to work with images, videos, and webcam feeds.)</p><p name="f612" id="f612" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2e. Configure PYTHONPATH environment variable</strong></p><p name="efb6" id="efb6" class="graf graf--p graf-after--p">A PYTHONPATH variable must be created that points to the \models, \models\research, and \models\research\slim directories. Do this by issuing the following commands (from any directory):</p><pre name="69a7" id="69a7" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\&gt; set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim</code></pre><p name="6615" id="6615" class="graf graf--p graf-after--pre">(Note: Every time the “tensorflow1” virtual environment is exited, the PYTHONPATH variable is reset and needs to be set up again. You can use “echo %PYTHONPATH% to see if it has been set or not.)</p><p name="666a" id="666a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2f. Compile Protobufs and run setup.py</strong></p><p name="fe23" id="fe23" class="graf graf--p graf-after--p">Next, compile the Protobuf files, which are used by TensorFlow to configure model and training parameters. Unfortunately, the short protoc compilation command posted on TensorFlow’s Object Detection API <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">installation page</a> does not work on Windows. Every .proto file in the \object_detection\protos directory must be called out individually by the command.</p><p name="4563" id="4563" class="graf graf--p graf-after--p">In the Anaconda Command Prompt, change directories to the \models\research directory:</p><pre name="07c7" id="07c7" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\&gt; cd C:\tensorflow1\models\research</code></pre><p name="b2b9" id="b2b9" class="graf graf--p graf-after--pre">Then copy and paste the following command into the command line and press Enter:</p><pre name="1142" id="1142" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">protoc --python_out=. .\object_detection\protos\anchor_generator.proto .\object_detection\protos\argmax_matcher.proto .\object_detection\protos\bipartite_matcher.proto .\object_detection\protos\box_coder.proto .\object_detection\protos\box_predictor.proto .\object_detection\protos\eval.proto .\object_detection\protos\faster_rcnn.proto .\object_detection\protos\faster_rcnn_box_coder.proto .\object_detection\protos\grid_anchor_generator.proto .\object_detection\protos\hyperparams.proto .\object_detection\protos\image_resizer.proto .\object_detection\protos\input_reader.proto .\object_detection\protos\losses.proto .\object_detection\protos\matcher.proto .\object_detection\protos\mean_stddev_box_coder.proto .\object_detection\protos\model.proto .\object_detection\protos\optimizer.proto .\object_detection\protos\pipeline.proto .\object_detection\protos\post_processing.proto .\object_detection\protos\preprocessor.proto .\object_detection\protos\region_similarity_calculator.proto .\object_detection\protos\square_box_coder.proto .\object_detection\protos\ssd.proto .\object_detection\protos\ssd_anchor_generator.proto .\object_detection\protos\string_int_label_map.proto .\object_detection\protos\train.proto .\object_detection\protos\keypoint_box_coder.proto .\object_detection\protos\multiscale_anchor_generator.proto .\object_detection\protos\graph_rewriter.proto .\object_detection\protos\calibration.proto .\object_detection\protos\flexible_grid_anchor_generator.proto</code></pre><p name="3fe6" id="3fe6" class="graf graf--p graf-after--pre">This creates a name_pb2.py file from every name.proto file in the \object_detection\protos folder.</p><p name="d0be" id="d0be" class="graf graf--p graf-after--p">(Note: TensorFlow occassionally adds new .proto files to the \protos folder. If you get an error saying ImportError: cannot import name ‘something_something_pb2’ , you may need to update the protoc command to include the new .proto files.)</p><p name="8ea6" id="8ea6" class="graf graf--p graf-after--p">Finally, run the following commands from the C:\tensorflow1\models\research directory:</p><pre name="7f65" id="7f65" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\tensorflow1\models\research&gt; python setup.py build<br>(tensorflow1) C:\tensorflow1\models\research&gt; python setup.py install</code></pre><p name="4690" id="4690" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">2g. Test TensorFlow setup to verify it works</strong></p><p name="e22b" id="e22b" class="graf graf--p graf-after--p">The TensorFlow Object Detection API is now all set up to use pre-trained models for object detection, or to train a new one. You can test it out and verify your installation is working by launching the object_detection_tutorial.ipynb script with Jupyter. From the \object_detection directory, issue this command:</p><pre name="bd75" id="bd75" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\tensorflow1\models\research\object_detection&gt; jupyter notebook object_detection_tutorial.ipynb</code></pre><p name="e897" id="e897" class="graf graf--p graf-after--pre">This opens the script in your default web browser and allows you to step through the code one section at a time. You can step through each section by clicking the “Run” button in the upper toolbar. The section is done running when the “In [ * ]” text next to the section populates with a number (e.g. “In [1]”).</p><p name="2303" id="2303" class="graf graf--p graf-after--p">(Note: part of the script downloads the ssd_mobilenet_v1 model from GitHub, which is about 74MB. This means it will take some time to complete the section, so be patient.)</p><p name="76e9" id="76e9" class="graf graf--p graf-after--p">Once you have stepped all the way through the script, you should see two labeled images at the bottom section the page. If you see this, then everything is working properly! If not, the bottom section will report any errors encountered. See the <a href="https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#appendix-common-errors" data-href="https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#appendix-common-errors" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Appendix</a> for a list of errors I encountered while setting this up.</p><p name="97fd" id="97fd" class="graf graf--p graf-after--p">Note: If you run the full Jupyter Notebook without getting any errors, but the labeled pictures still don’t appear, try this: go in to object_detection/utils/visualization_utils.py and comment out the import statements around lines 29 and 30 that include matplotlib. Then, try re-running the Jupyter notebook.</p><figure name="b553" id="b553" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 576px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 82.19999999999999%;"></div><img class="graf-image" data-image-id="1*MVDm8woFTYfxPcjXym52Yw.jpeg" data-width="934" data-height="768" src="https://cdn-images-1.medium.com/max/800/1*MVDm8woFTYfxPcjXym52Yw.jpeg"></div></figure><h3 name="1d4c" id="1d4c" class="graf graf--h3 graf-after--figure">3. Gather and Label Pictures</h3><p name="29db" id="29db" class="graf graf--p graf-after--h3">Now that the TensorFlow Object Detection API is all set up and ready to go, we need to provide the images it will use to train a new detection classifier.</p><h4 name="bc14" id="bc14" class="graf graf--h4 graf-after--p">3a. Gather Pictures</h4><p name="dd9e" id="dd9e" class="graf graf--p graf-after--h4">TensorFlow needs hundreds of images of an object to train a good detection classifier. To train a robust classifier, the training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions. There should be some images where the desired object is partially obscured, overlapped with something else, or only halfway in the picture.</p><p name="a98b" id="a98b" class="graf graf--p graf-after--p">For my Pinochle Card Detection classifier, I have six different objects I want to detect (the card ranks nine, ten, jack, queen, king, and ace — I am not trying to detect suit, just rank). I used my iPhone to take about 40 pictures of each card on its own, with various other non-desired objects in the pictures. Then, I took about another 100 pictures with multiple cards in the picture. I know I want to be able to detect the cards when they’re overlapping, so I made sure to have the cards be overlapped in many images.</p><figure name="6c5f" id="6c5f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 701px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100.1%;"></div><img class="graf-image" data-image-id="1*Iu1YlweumtGpv8-DVv1IRw.jpeg" data-width="793" data-height="794" src="https://cdn-images-1.medium.com/max/800/1*Iu1YlweumtGpv8-DVv1IRw.jpeg"></div></figure><p name="447d" id="447d" class="graf graf--p graf-after--figure">You can use your phone to take pictures of the objects or download images of the objects from Google Image Search. I recommend having at least 200 pictures overall. I used 311 pictures to train my card detector.</p><p name="ffb2" id="ffb2" class="graf graf--p graf-after--p">Make sure the images aren’t too large. They should be less than 200KB each, and their resolution shouldn’t be more than 720x1280. The larger the images are, the longer it will take to train the classifier. You can use the resizer.py script in this repository to reduce the size of the images.</p><p name="ce33" id="ce33" class="graf graf--p graf-after--p">After you have all the pictures you need, move 20% of them to the \object_detection\images\test directory, and 80% of them to the \object_detection\images\train directory. Make sure there are a variety of pictures in both the \test and \train directories.</p><h4 name="f5ee" id="f5ee" class="graf graf--h4 graf-after--p">3b. Label Pictures</h4><p name="fa9a" id="fa9a" class="graf graf--p graf-after--h4">Here comes the fun part! With all the pictures gathered, it’s time to label the desired objects in every picture. LabelImg is a great tool for labeling images, and its GitHub page has very clear instructions on how to install and use it.</p><p name="4038" id="4038" class="graf graf--p graf-after--p"><a href="https://github.com/tzutalin/labelImg" data-href="https://github.com/tzutalin/labelImg" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LabelImg GitHub link</a></p><p name="f7e5" id="f7e5" class="graf graf--p graf-after--p"><a href="https://www.dropbox.com/s/tq7zfrcwl44vxan/windows_v1.6.0.zip?dl=1" data-href="https://www.dropbox.com/s/tq7zfrcwl44vxan/windows_v1.6.0.zip?dl=1" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">LabelImg download link</a></p><p name="daa6" id="daa6" class="graf graf--p graf-after--p">Download and install LabelImg, point it to your \images\train directory, and then draw a box around each object in each image. Repeat the process for all the images in the \images\test directory. This will take a while!</p><figure name="a835" id="a835" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 380px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.300000000000004%;"></div><img class="graf-image" data-image-id="1*7VxK4a1WfUibU64dH8k2QQ.jpeg" data-width="960" data-height="521" src="https://cdn-images-1.medium.com/max/800/1*7VxK4a1WfUibU64dH8k2QQ.jpeg"></div></figure><p name="74f0" id="74f0" class="graf graf--p graf-after--figure">LabelImg saves a .xml file containing the label data for each image. These .xml files will be used to generate TFRecords, which are one of the inputs to the TensorFlow trainer. Once you have labeled and saved each image, there will be one .xml file for each image in the \test and \train directories.</p><h3 name="2453" id="2453" class="graf graf--h3 graf-after--p">4. Generate Training Data</h3><p name="1722" id="1722" class="graf graf--p graf-after--h3">With the images labeled, it’s time to generate the TFRecords that serve as input data to the TensorFlow training model. This article uses the xml_to_csv.py and generate_tfrecord.py scripts from <a href="https://github.com/datitran/raccoon_dataset" data-href="https://github.com/datitran/raccoon_dataset" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dat Tran’s Raccoon Detector dataset</a>, with some slight modifications to work with our directory structure.</p><p name="c118" id="c118" class="graf graf--p graf-after--p">First, the image .xml data will be used to create .csv files containing all the data for the train and test images. From the \object_detection folder, issue the following command in the Anaconda command prompt:</p><pre name="ca1e" id="ca1e" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\tensorflow1\models\research\object_detection&gt; python xml_to_csv.py</code></pre><p name="c2a0" id="c2a0" class="graf graf--p graf-after--pre">This creates a train_labels.csv and test_labels.csv file in the \object_detection\images folder.</p><p name="48b5" id="48b5" class="graf graf--p graf-after--p">Next, open the generate_tfrecord.py file in a text editor. Replace the label map starting at line 31 with your own label map, where each object is assigned an ID number. This same number assignment will be used when configuring the labelmap.pbtxt file in Step 5b.</p><p name="c605" id="c605" class="graf graf--p graf-after--p">For example, say you are training a classifier to detect basketballs, shirts, and shoes. You will replace the following code in generate_tfrecord.py:</p><pre name="5a29" id="5a29" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code"># TO-DO replace this with label map<br>def class_text_to_int(row_label):<br>    if row_label == &#39;nine&#39;:<br>        return 1<br>    elif row_label == &#39;ten&#39;:<br>        return 2<br>    elif row_label == &#39;jack&#39;:<br>        return 3<br>    elif row_label == &#39;queen&#39;:<br>        return 4<br>    elif row_label == &#39;king&#39;:<br>        return 5<br>    elif row_label == &#39;ace&#39;:<br>        return 6<br>    else:<br>        None</code></pre><p name="c35d" id="c35d" class="graf graf--p graf-after--pre">With this:</p><pre name="6702" id="6702" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code"># TO-DO replace this with label map<br>def class_text_to_int(row_label):<br>    if row_label == &#39;basketball&#39;:<br>        return 1<br>    elif row_label == &#39;shirt&#39;:<br>        return 2<br>    elif row_label == &#39;shoe&#39;:<br>        return 3<br>    else:<br>        None</code></pre><p name="bbe9" id="bbe9" class="graf graf--p graf-after--pre">Then, generate the TFRecord files by issuing these commands from the \object_detection folder:</p><pre name="bb4a" id="bb4a" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record<br>python generate_tfrecord.py --csv_input=images\test_labels.csv --image_dir=images\test --output_path=test.record</code></pre><p name="2b99" id="2b99" class="graf graf--p graf-after--pre">These generate a train.record and a test.record file in \object_detection. These will be used to train the new object detection classifier.</p><h3 name="67d4" id="67d4" class="graf graf--h3 graf-after--p">5. Create Label Map and Configure Training</h3><p name="7af3" id="7af3" class="graf graf--p graf-after--h3">The last thing to do before training is to create a label map and edit the training configuration file.</p><h4 name="022e" id="022e" class="graf graf--h4 graf-after--p">5a. Label map</h4><p name="3578" id="3578" class="graf graf--p graf-after--h4">The label map tells the trainer what each object is by defining a mapping of class names to class ID numbers. Use a text editor to create a new file and save it as labelmap.pbtxt in the C:\tensorflow1\models\research\object_detection\training folder. (Make sure the file type is .pbtxt, not .txt !) In the text editor, copy or type in the label map in the format below (the example below is the label map for my Pinochle Deck Card Detector):</p><pre name="00ab" id="00ab" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">item {<br>  id: 1<br>  name: &#39;nine&#39;<br>}</code></pre><pre name="0c91" id="0c91" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 2<br>  name: &#39;ten&#39;<br>}</code></pre><pre name="0778" id="0778" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 3<br>  name: &#39;jack&#39;<br>}</code></pre><pre name="40d6" id="40d6" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 4<br>  name: &#39;queen&#39;<br>}</code></pre><pre name="7d19" id="7d19" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 5<br>  name: &#39;king&#39;<br>}</code></pre><pre name="f3c6" id="f3c6" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 6<br>  name: &#39;ace&#39;<br>}</code></pre><p name="abd8" id="abd8" class="graf graf--p graf-after--pre">The label map ID numbers should be the same as what is defined in the generate_tfrecord.py file. For the basketball, shirt, and shoe detector example mentioned in Step 4, the labelmap.pbtxt file will look like:</p><pre name="939e" id="939e" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">item {<br>  id: 1<br>  name: &#39;basketball&#39;<br>}</code></pre><pre name="7d81" id="7d81" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 2<br>  name: &#39;shirt&#39;<br>}</code></pre><pre name="e302" id="e302" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">item {<br>  id: 3<br>  name: &#39;shoe&#39;<br>}</code></pre><h4 name="1860" id="1860" class="graf graf--h4 graf-after--pre">5b. Configure training</h4><p name="cda0" id="cda0" class="graf graf--p graf-after--h4">Finally, the object detection training pipeline must be configured. It defines which model and what parameters will be used for training. This is the last step before running training!</p><p name="c833" id="c833" class="graf graf--p graf-after--p">Navigate to C:\tensorflow1\models\research\object_detection\samples\configs and copy the faster_rcnn_inception_v2_pets.config file into the \object_detection\training directory. Then, open the file with a text editor. There are several changes to make to the .config file, mainly changing the number of classes and examples, and adding the file paths to the training data.</p><p name="4c6d" id="4c6d" class="graf graf--p graf-after--p">Make the following changes to the faster_rcnn_inception_v2_pets.config file. Note: The paths must be entered with single forward slashes (NOT backslashes), or TensorFlow will give a file path error when trying to train the model! Also, the paths must be in double quotation marks ( “ ), not single quotation marks ( ‘ ).</p><ul class="postList"><li name="6407" id="6407" class="graf graf--li graf-after--p">Line 9. Change num_classes to the number of different objects you want the classifier to detect. For the above basketball, shirt, and shoe detector, it would be num_classes : 3 .</li><li name="22c9" id="22c9" class="graf graf--li graf-after--li">Line 106. Change fine_tune_checkpoint to:</li><li name="8503" id="8503" class="graf graf--li graf-after--li">fine_tune_checkpoint : “C:/tensorflow1/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt”</li><li name="3c08" id="3c08" class="graf graf--li graf-after--li">Lines 123 and 125. In the train_input_reader section, change input_path and label_map_path to:</li><li name="688c" id="688c" class="graf graf--li graf-after--li">input_path : “C:/tensorflow1/models/research/object_detection/train.record”</li><li name="6be5" id="6be5" class="graf graf--li graf-after--li">label_map_path: “C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt”</li><li name="d358" id="d358" class="graf graf--li graf-after--li">Line 130. Change num_examples to the number of images you have in the \images\test directory.</li><li name="1f86" id="1f86" class="graf graf--li graf-after--li">Lines 135 and 137. In the eval_input_reader section, change input_path and label_map_path to:</li><li name="dee8" id="dee8" class="graf graf--li graf-after--li">input_path : “C:/tensorflow1/models/research/object_detection/test.record”</li><li name="6880" id="6880" class="graf graf--li graf-after--li">label_map_path: “C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt”</li></ul><p name="ead7" id="ead7" class="graf graf--p graf-after--li">Save the file after the changes have been made. That’s it! The training job is all configured and ready to go!</p><h3 name="c936" id="c936" class="graf graf--h3 graf-after--p">6. Run the Training</h3><p name="aff5" id="aff5" class="graf graf--p graf-after--h3">UPDATE 9/26/18: <em class="markup--em markup--p-em">As of version 1.9, TensorFlow has deprecated the “train.py” file and replaced it with “model_main.py” file. I haven’t been able to get model_main.py to work correctly yet (I run in to errors related to pycocotools). Fortunately, the train.py file is still available in the /object_detection/legacy folder. Simply move train.py from /object_detection/legacy into the /object_detection folder and then continue following the steps below.</em></p><p name="fd47" id="fd47" class="graf graf--p graf-after--p">Here we go! From the \object_detection directory, issue the following command to begin training:</p><pre name="d8ce" id="d8ce" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config</code></pre><p name="9a2d" id="9a2d" class="graf graf--p graf-after--pre">If everything has been set up correctly, TensorFlow will initialize the training. The initialization can take up to 30 seconds before the actual training begins. When training begins, it will look like this:</p><figure name="ebb2" id="ebb2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 366px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.300000000000004%;"></div><img class="graf-image" data-image-id="1*CjzE9hwGbMOq0yHjGj0PdA.jpeg" data-width="979" data-height="512" src="https://cdn-images-1.medium.com/max/800/1*CjzE9hwGbMOq0yHjGj0PdA.jpeg"></div></figure><p name="387a" id="387a" class="graf graf--p graf-after--figure">Each step of training reports the loss. It will start high and get lower and lower as training progresses. For my training on the Faster-RCNN-Inception-V2 model, it started at about 3.0 and quickly dropped below 0.8. I recommend allowing your model to train until the loss consistently drops below 0.05, which will take about 40,000 steps, or about 2 hours (depending on how powerful your CPU and GPU are). Note: The loss numbers will be different if a different model is used. MobileNet-SSD starts with a loss of about 20, and should be trained until the loss is consistently under 2.</p><p name="1ed4" id="1ed4" class="graf graf--p graf-after--p">You can view the progress of the training job by using TensorBoard. To do this, open a new instance of Anaconda Prompt, activate the tensorflow1 virtual environment, change to the C:\tensorflow1\models\research\object_detection directory, and issue the following command:</p><pre name="c000" id="c000" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;tensorboard --logdir=training</code></pre><p name="fa14" id="fa14" class="graf graf--p graf-after--pre">This will create a webpage on your local machine at YourPCName:6006, which can be viewed through a web browser. The TensorBoard page provides information and graphs that show how the training is progressing. One important graph is the Loss graph, which shows the overall loss of the classifier over time.</p><figure name="9d03" id="9d03" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 341px; max-height: 273px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.10000000000001%;"></div><img class="graf-image" data-image-id="1*rewHKt6DebvwCIBhnzFxog.jpeg" data-width="341" data-height="273" src="https://cdn-images-1.medium.com/max/800/1*rewHKt6DebvwCIBhnzFxog.jpeg"></div></figure><p name="db22" id="db22" class="graf graf--p graf-after--figure">The training routine periodically saves checkpoints about every five minutes. You can terminate the training by pressing Ctrl+C while in the command prompt window. I typically wait until just after a checkpoint has been saved to terminate the training. You can terminate training and start it later, and it will restart from the last saved checkpoint. The checkpoint at the highest number of steps will be used to generate the frozen inference graph.</p><h3 name="a2b5" id="a2b5" class="graf graf--h3 graf-after--p">7. Export Inference Graph</h3><p name="c779" id="c779" class="graf graf--p graf-after--h3">Now that training is complete, the last step is to generate the frozen inference graph (.pb file). From the \object_detection folder, issue the following command, where “XXXX” in “model.ckpt-XXXX” should be replaced with the highest-numbered .ckpt file in the training folder:</p><pre name="5f59" id="5f59" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory inference_graph</code></pre><p name="5189" id="5189" class="graf graf--p graf-after--pre">This creates a frozen_inference_graph.pb file in the \object_detection\inference_graph folder. The .pb file contains the object detection classifier.</p><h3 name="9123" id="9123" class="graf graf--h3 graf-after--p">8. Use Your Newly Trained Object Detection Classifier!</h3><p name="43d1" id="43d1" class="graf graf--p graf-after--h3">The object detection classifier is all ready to go! I’ve written Python scripts to test it out on an image, video, or webcam feed.</p><p name="a559" id="a559" class="graf graf--p graf-after--p">Before running the Python scripts, you need to modify the NUM_CLASSES variable in the script to equal the number of classes you want to detect. (For my Pinochle Card Detector, there are six cards I want to detect, so NUM_CLASSES = 6.)</p><p name="5641" id="5641" class="graf graf--p graf-after--p">To test your object detector, move a picture of the object or objects into the \object_detection folder, and change the IMAGE_NAME variable in the Object_detection_image.py to match the file name of the picture. Alternatively, you can use a video of the objects (using Object_detection_video.py), or just plug in a USB webcam and point it at the objects (using Object_detection_webcam.py).</p><p name="58f8" id="58f8" class="graf graf--p graf-after--p">To run any of the scripts, type “idle” in the Anaconda Command Prompt (with the “tensorflow1” virtual environment activated) and press ENTER. This will open IDLE, and from there, you can open any of the scripts and run them.</p><p name="319f" id="319f" class="graf graf--p graf-after--p">If everything is working properly, the object detector will initialize for about 10 seconds and then display a window showing any objects it’s detected in the image!</p><figure name="4dfd" id="4dfd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 383px; max-height: 511px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 133.4%;"></div><img class="graf-image" data-image-id="1*c_aCQtVXmoNnOiEM_Yra0Q.jpeg" data-width="383" data-height="511" src="https://cdn-images-1.medium.com/max/800/1*c_aCQtVXmoNnOiEM_Yra0Q.jpeg"></div></figure><p name="2e9e" id="2e9e" class="graf graf--p graf-after--figure">If you encounter errors, please check out the Appendix: it has a list of errors that I ran in to while setting up my object detection classifier. You can also trying Googling the error. There is usually useful information on Stack Exchange or in TensorFlow’s Issues on GitHub.</p><h3 name="9777" id="9777" class="graf graf--h3 graf-after--p">Appendix: Common Errors</h3><p name="0f30" id="0f30" class="graf graf--p graf-after--h3">It appears that the TensorFlow Object Detection API was developed on a Linux-based operating system, and most of the directions given by the documentation are for a Linux OS. Trying to get a Linux-developed software library to work on Windows can be challenging. There are many little snags that I ran in to while trying to set up tensorflow-gpu to train an object detection classifier on Windows 10. This Appendix is a list of errors I ran in to, and their resolutions.</p><h4 name="8b62" id="8b62" class="graf graf--h4 graf-after--p">1. ModuleNotFoundError: No module named ‘deployment’ or No module named ‘nets’</h4><p name="1c20" id="1c20" class="graf graf--p graf-after--h4">This error occurs when you try to run object_detection_tutorial.ipynb or train.py and you don’t have the PATH and PYTHONPATH environment variables set up correctly. Exit the virtual environment by closing and re-opening the Anaconda Prompt window. Then, issue “activate tensorflow1” to re-enter the environment, and then issue the commands given in Step 2e.</p><p name="ff65" id="ff65" class="graf graf--p graf-after--p">You can use “echo %PATH%” and “echo %PYTHONPATH%” to check the environment variables and make sure they are set up correctly.</p><p name="7fb6" id="7fb6" class="graf graf--p graf-after--p">Also, make sure you have run these commands from the \models\research directory:</p><pre name="a40a" id="a40a" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">setup.py build<br>setup.py install</code></pre><h4 name="8654" id="8654" class="graf graf--h4 graf-after--pre">2. ImportError: cannot import name ‘preprocessor_pb2’</h4><h4 name="3d65" id="3d65" class="graf graf--h4 graf-after--h4">ImportError: cannot import name ‘string_int_label_map_pb2’</h4><h4 name="ae31" id="ae31" class="graf graf--h4 graf-after--h4">(or similar errors with other pb2 files)</h4><p name="5eb7" id="5eb7" class="graf graf--p graf-after--h4">This occurs when the protobuf files (in this case, preprocessor.proto) have not been compiled. Re-run the protoc command given in Step 2f. Check the \object_detection\protos folder to make sure there is a name_pb2.py file for every name.proto file.</p><h4 name="3298" id="3298" class="graf graf--h4 graf-after--p">3. object_detection/protos/.proto: No such file or directory</h4><p name="22b9" id="22b9" class="graf graf--p graf-after--h4">This occurs when you try to run the</p><pre name="d88a" id="d88a" class="graf graf--pre graf--startsWithDoubleQuote graf-after--p"><code class="markup--code markup--pre-code">“protoc object_detection/protos/*.proto --python_out=.”</code></pre><p name="3368" id="3368" class="graf graf--p graf-after--pre">command given on the TensorFlow Object Detection API installation page. Sorry, it doesn’t work on Windows! Copy and paste the full command given in Step 2f instead. There’s probably a more graceful way to do it, but I don’t know what it is.</p><h4 name="3a95" id="3a95" class="graf graf--h4 graf-after--p">4. Unsuccessful TensorSliceReader constructor: Failed to get “file path” … The filename, directory name, or volume label syntax is incorrect.</h4><p name="0f37" id="0f37" class="graf graf--p graf-after--h4">This error occurs when the filepaths in the training configuration file (faster_rcnn_inception_v2_pets.config or similar) have not been entered with backslashes instead of forward slashes. Open the .config file and make sure all file paths are given in the following format:</p><pre name="4bda" id="4bda" class="graf graf--pre graf--startsWithDoubleQuote graf-after--p"><code class="markup--code markup--pre-code">“C:/path/to/model.file”</code></pre><h4 name="14a9" id="14a9" class="graf graf--h4 graf-after--pre">5. ValueError: Tried to convert ‘t’ to a tensor and failed. Error: Argument must be a dense tensor: range(0, 3) — got shape [3], but wanted [].</h4><p name="b10e" id="b10e" class="graf graf--p graf-after--h4">The issue is with models/research/object_detection/utils/learning_schedules.py Currently it is</p><pre name="bf11" id="bf11" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),<br>                                      range(num_boundaries),<br>                                      [0] * num_boundaries))</code></pre><p name="4aa5" id="4aa5" class="graf graf--p graf-after--pre">Wrap list() around the range() like this:</p><pre name="d572" id="d572" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),<br>                                     list(range(num_boundaries)),<br>                                      [0] * num_boundaries))</code></pre><p name="9049" id="9049" class="graf graf--p graf-after--pre"><a href="https://github.com/tensorflow/models/issues/3705#issuecomment-375563179" data-href="https://github.com/tensorflow/models/issues/3705#issuecomment-375563179" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ref: Tensorflow Issue#3705</a></p><h4 name="ffe4" id="ffe4" class="graf graf--h4 graf-after--p">6. ImportError: DLL load failed: The specified procedure could not be found. (or other DLL-related errors)</h4><p name="432b" id="432b" class="graf graf--p graf-after--h4">This error occurs because the CUDA and cuDNN versions you have installed are not compatible with the version of TensorFlow you are using. The easiest way to resolve this error is to use Anaconda’s cudatoolkit package rather than manually installing CUDA and cuDNN. If you ran into these errors, try creating a new Anaconda virtual environment:</p><pre name="cc29" id="cc29" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">conda create -n tensorflow2 pip python=3.5</code></pre><p name="8050" id="8050" class="graf graf--p graf-after--pre">Then, once inside the environment, install TensorFlow using CONDA rather than PIP:</p><pre name="da02" id="da02" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">conda install tensorflow-gpu</code></pre><p name="e4fb" id="e4fb" class="graf graf--p graf-after--pre">Then restart this guide from Step 2 (but you can skip the part where you install TensorFlow in Step 2d).</p><h4 name="7d77" id="7d77" class="graf graf--h4 graf-after--p">7. In Step 2g, the Jupyter Notebook runs all the way through with no errors, but no pictures are displayed at the end.</h4><p name="dcc8" id="dcc8" class="graf graf--p graf-after--h4">If you run the full Jupyter Notebook without getting any errors, but the labeled pictures still don’t appear, try this: go in to object_detection/utils/visualization_utils.py and comment out the import statements around lines 29 and 30 that include matplotlib. Then, try re-running the Jupyter notebook. (The visualization_utils.py script changes quite a bit, so it might not be exactly line 29 and 30.)</p><p name="4230" id="4230" class="graf graf--p graf-after--p">I hope this will help you with to To Train an Object Detection Classifier for Multiple Objects Using TensorFlow (GPU) on Windows 10.</p><p name="d729" id="d729" class="graf graf--p graf-after--p graf--trailing">Thank you for reaching out.</p></div></div></section>
</section>
</article></body></html>