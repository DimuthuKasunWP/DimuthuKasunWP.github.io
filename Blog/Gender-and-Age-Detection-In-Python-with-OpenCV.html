<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Gender and Age Detection In Python with OpenCV</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Gender and Age Detection In Python with OpenCV</h1>
</header>
<section data-field="subtitle" class="p-summary">
Computer Vision is the field of study that enables computers to see and identify digital images and videos as a human would. The…
</section>
<section data-field="body" class="e-content">
<section name="9d9d" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="607f" id="607f" class="graf graf--h3 graf--leading graf--title">Gender and Age Detection In Python with OpenCV</h3><figure name="1fd5" id="1fd5" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 466px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.60000000000001%;"></div><img class="graf-image" data-image-id="1*u8jdhMkct5VZ1Z4mGkmstA.jpeg" data-width="1280" data-height="853" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*u8jdhMkct5VZ1Z4mGkmstA.jpeg"></div></figure><p name="459a" id="459a" class="graf graf--p graf-after--figure">Computer Vision is a field of study that enables computers to see and identify digital images and videos as a human would. The challenges it faces largely follow from the limited understanding of the biological vision. Computer Vision involves acquiring, processing, analyzing, and understanding digital images to extract high-dimensional data from the real world in order to generate symbolic or numerical information that can then be used to make decisions. The process often includes practices like object recognition, video tracking, motion estimation, and image restoration.</p><h4 name="0832" id="0832" class="graf graf--h4 graf-after--p">What is OpenCV?</h4><p name="85c0" id="85c0" class="graf graf--p graf-after--h4">OpenCV is short for Open Source Computer Vision. Intuitively by the name, it is an open-source Computer Vision and Machine Learning library. This library is capable of processing real-time images and videos while also boasting analytical capabilities. It supports the Deep Learning frameworks <a href="https://data-flair.training/blogs/tensorflow-tutorials-home/" data-href="https://data-flair.training/blogs/tensorflow-tutorials-home/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">TensorFlow</em></a>, Caffe, and PyTorch.</p><h4 name="4021" id="4021" class="graf graf--h4 graf-after--p">What is a CNN?</h4><p name="54e3" id="54e3" class="graf graf--p graf-after--h4">A <em class="markup--em markup--p-em">Convolutional Neural Network</em> is a deep neural network (DNN) widely used for the purposes of image recognition and processing and <em class="markup--em markup--p-em">NLP</em>. Also known as a ConvNet, CNN has input and output layers, and multiple hidden layers, many of which are convolutional. In a way, CNNs are regularized multilayer perceptrons.</p><h4 name="cc07" id="cc07" class="graf graf--h4 graf-after--p">Gender and Age Detection Python Project- Objective</h4><p name="ce6a" id="ce6a" class="graf graf--p graf-after--h4">To build a gender and age detector that can approximately guess the gender and age of the person (face) in a picture using <em class="markup--em markup--p-em">Deep Learning</em> on the Adience dataset.</p><h4 name="734c" id="734c" class="graf graf--h4 graf-after--p">Gender and Age Detection — About the Project</h4><p name="6e11" id="6e11" class="graf graf--p graf-after--h4">In this Python Project, I will use Deep Learning to accurately identify the gender and age of a person from a single image of a face. I will use the models trained by Tal Hassner and Gil Levi. The predicted gender may be one of ‘Male’ and ‘Female’, and the predicted age may be one of the following ranges- (0–2), (4–6), (8–12), (15–20), (25–32), (38–43), (48–53), (60–100) (8 nodes in the final softmax layer). It is very difficult to accurately guess the exact age from a single image because of factors like makeup, lighting, obstructions, and facial expressions. And so, I make this a classification problem instead of making it one of regression.</p><h4 name="cf4c" id="cf4c" class="graf graf--h4 graf-after--p">The CNN Architecture</h4><p name="c96e" id="c96e" class="graf graf--p graf-after--h4">The convolutional neural network for this python project has 3 convolutional layers.similar to the CaffeNet and AlexNet. The network uses 3 convolutional layers, 2 fully connected layers, and a final output layer. The details of the layers are given below.</p><ul class="postList"><li name="69a7" id="69a7" class="graf graf--li graf-after--p">Convolutional layer; 96 nodes, kernel size 7</li><li name="b7be" id="b7be" class="graf graf--li graf-after--li">Convolutional layer; 256 nodes, kernel size 5</li><li name="a776" id="a776" class="graf graf--li graf-after--li">Convolutional layer; 384 nodes, kernel size 3</li></ul><p name="506c" id="506c" class="graf graf--p graf-after--li">It has 2 fully connected layers, each with 512 nodes, and a final output layer of softmax type.</p><p name="7570" id="7570" class="graf graf--p graf-after--p">To go about the python project, I’ll:</p><ul class="postList"><li name="4fb6" id="4fb6" class="graf graf--li graf-after--p">Detect faces</li><li name="ab3a" id="ab3a" class="graf graf--li graf-after--li">Classify into Male/Female</li><li name="d1f4" id="d1f4" class="graf graf--li graf-after--li">Classify into one of the 8 age ranges</li><li name="8734" id="8734" class="graf graf--li graf-after--li">Put the results on the image and display it</li></ul><h4 name="44f3" id="44f3" class="graf graf--h4 graf-after--li">The Dataset</h4><p name="41d5" id="41d5" class="graf graf--p graf-after--h4">For this python project, I’ll use the Adience dataset; the dataset is available in the public domain and you can find it <a href="https://www.kaggle.com/ttungl/adience-benchmark-gender-and-age-classification" data-href="https://www.kaggle.com/ttungl/adience-benchmark-gender-and-age-classification" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">here</em></a>. This dataset serves as a benchmark for face photos and is inclusive of various real-world imaging conditions like noise, lighting, pose, and appearance. The images have been collected from Flickr albums and distributed under the Creative Commons (CC) license. It has a total of 26,580 photos of 2,284 subjects in eight age ranges (as mentioned above) and is about 1GB in size. The models I will use have been trained on this dataset.</p><p name="95e1" id="95e1" class="graf graf--p graf-after--p">Also, If you want different dataset I suggest <a href="https://talhassner.github.io/home/projects/Adience/Adience-data.html" data-href="https://talhassner.github.io/home/projects/Adience/Adience-data.html" class="markup--anchor markup--p-anchor" rel="noreferrer noopener noopener" target="_blank">Adience dataset</a> for training the model.</p><h4 name="c007" id="c007" class="graf graf--h4 graf-after--p">Prerequisites</h4><p name="8f30" id="8f30" class="graf graf--p graf-after--h4">You’ll need to install OpenCV (cv2) to be able to run this project. You can do this with pip-</p><pre name="9c9f" id="9c9f" class="graf graf--pre graf-after--p">pip install opencv-python</pre><p name="2c55" id="2c55" class="graf graf--p graf-after--pre">Other packages you’ll be needing are math and argparse, but those come as part of the standard Python library.</p><h3 name="c332" id="c332" class="graf graf--h3 graf-after--p">Steps for practicing gender and age detection python project</h3><ol class="postList"><li name="c4c6" id="c4c6" class="graf graf--li graf-after--h3"><a href="https://drive.google.com/file/d/1yy_poZSFAPKi0y2e2yj9XDe1N8xXYuKB/view" data-href="https://drive.google.com/file/d/1yy_poZSFAPKi0y2e2yj9XDe1N8xXYuKB/view" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Download this zip</a>. Unzip it and put its contents in a directory you’ll call gad.</li></ol><p name="ac31" id="ac31" class="graf graf--p graf-after--li">The contents of this zip are:</p><ul class="postList"><li name="0f03" id="0f03" class="graf graf--li graf-after--p">opencv_face_detector.pbtxt</li><li name="9b09" id="9b09" class="graf graf--li graf-after--li">opencv_face_detector_uint8.pb</li><li name="b893" id="b893" class="graf graf--li graf-after--li">age_deploy.prototxt</li><li name="3048" id="3048" class="graf graf--li graf-after--li">age_net.caffemodel</li><li name="309a" id="309a" class="graf graf--li graf-after--li">gender_deploy.prototxt</li><li name="bfcb" id="bfcb" class="graf graf--li graf-after--li">gender_net.caffemodel</li><li name="395e" id="395e" class="graf graf--li graf-after--li">a few pictures to try the project on</li></ul><p name="1de9" id="1de9" class="graf graf--p graf-after--li">For face detection, I have a .pb file- this is a protobuf file (protocol buffer); it holds the graph definition and the trained weights of the model. I can use this to run the trained model. And while a .pb file holds the protobuf in binary format, one with the .pbtxt extension holds it in text format. These are TensorFlow files. For age and gender, the .prototxt files describe the network configuration and the .caffemodel file defines the internal states of the parameters of the layers.</p><p name="33b4" id="33b4" class="graf graf--p graf-after--p">2. I use the argparse library to create an argument parser so we can get the image argument from the command prompt. I make it parse the argument holding the path to the image to classify gender and age for.</p><p name="c7fe" id="c7fe" class="graf graf--p graf-after--p">3. For the face, age, and gender, initialize protocol buffer, and model.</p><p name="5d1e" id="5d1e" class="graf graf--p graf-after--p">4. Initialize the mean values for the model and the lists of age ranges and genders to classify from.</p><p name="3aa4" id="3aa4" class="graf graf--p graf-after--p">5. Now, use the readNet() method to load the networks. The first parameter holds trained weights and the second carries network configuration.</p><p name="a6dd" id="a6dd" class="graf graf--p graf-after--p">6. Let’s capture video stream in case you’d like to classify it on a webcam’s stream. Set padding to 20.</p><p name="03d4" id="03d4" class="graf graf--p graf-after--p">7. Now until any key is pressed, I read the stream and store the content into the names hasFrame and frame. If it isn’t a video, it must wait, and so we call up waitKey() from cv2, then break.</p><p name="ec35" id="ec35" class="graf graf--p graf-after--p">8. Let’s make a call to the highlightFace() function with the faceNet and frame parameters, and what this returns, I will store in the names resultImg and faceBoxes. And if I got 0 faceBoxes, it means there was no face to detect.<br>Here, net is faceNet- this model is the DNN Face Detector and holds only about 2.7MB on disk.</p><ul class="postList"><li name="ed21" id="ed21" class="graf graf--li graf-after--p">Create a shallow copy of the frame and get its height and width.</li><li name="619a" id="619a" class="graf graf--li graf-after--li">Create a blob from the shallow copy.</li><li name="0704" id="0704" class="graf graf--li graf-after--li">Set the input and make a forward pass to the network.</li><li name="1375" id="1375" class="graf graf--li graf-after--li">faceBoxes is an empty list now. for each value in 0 to 127, define the confidence (between 0 and 1). Wherever I find the confidence greater than the confidence threshold, which is 0.7, we get the x1, y1, x2, and y2 coordinates and append a list of those to faceBoxes.</li><li name="eb5d" id="eb5d" class="graf graf--li graf-after--li">Then, I put up rectangles on the image for each such list of coordinates and return two things: the shallow copy and the list of faceBoxes.</li></ul><p name="72ad" id="72ad" class="graf graf--p graf-after--li">9. But if there are indeed faceBoxes, for each of those, I define the face, create a 4-dimensional blob from the image. In doing this, I scale it, resize it, and pass in the mean values.</p><p name="13fd" id="13fd" class="graf graf--p graf-after--p">10. I feed the input and give the network a forward pass to get the confidence of the two classes. Whichever is higher, that is the gender of the person in the picture.</p><p name="15bb" id="15bb" class="graf graf--p graf-after--p">11. Then, I do the same thing for age.</p><p name="1f3c" id="1f3c" class="graf graf--p graf-after--p">12. I’ll add the gender and age texts to the resulting image and display it with imshow().</p><p name="3614" id="3614" class="graf graf--p graf-after--p">Let’s follow these steps slowly.</p><h3 name="132e" id="132e" class="graf graf--h3 graf-after--p">1.1. Gender Prediction</h3><p name="23b6" id="23b6" class="graf graf--p graf-after--h3">They have framed Gender Prediction as a classification problem. The output layer in the gender prediction network is of type softmax with 2 nodes indicating the two classes “Male” and “Female”.</p><h3 name="34dd" id="34dd" class="graf graf--h3 graf-after--p">1.2. Age Prediction</h3><p name="d8e2" id="d8e2" class="graf graf--p graf-after--h3">Ideally, Age Prediction should be approached as a Regression problem since we are expecting a real number as the output. However, estimating age accurately using regression is challenging. Even humans cannot accurately predict the age based on looking at a person. However, we have an idea of whether they are in their 20s or in their 30s. Because of this reason, it is wise to frame this problem as a classification problem where we try to estimate the age group the person is in. For example, age in the range of 0–2 is a single class, 4–6 is another class and so on.</p><p name="8bf7" id="8bf7" class="graf graf--p graf-after--p">The Adience dataset has 8 classes divided into the following age groups [(0–2), (4–6), (8–12), (15–20), (25–32), (38–43), (48–53), (60–100)]. Thus, the age prediction network has 8 nodes in the final softmax layer indicating the mentioned age ranges.</p><p name="23bf" id="23bf" class="graf graf--p graf-after--p">It should be kept in mind that Age prediction from a single image is not a very easy problem to solve as the perceived age depends on a lot of factors and people of the same age may look pretty different in various parts of the world. <em class="markup--em markup--p-em">Also, people try very hard to hide their real age!</em></p><h3 name="94f1" id="94f1" class="graf graf--h3 graf-after--p">2.How to Code</h3><p name="3a35" id="3a35" class="graf graf--p graf-after--h3">The code can be divided into four parts:</p><ol class="postList"><li name="a249" id="a249" class="graf graf--li graf-after--p">Detect Faces</li><li name="2f3b" id="2f3b" class="graf graf--li graf-after--li">Detect Gender</li><li name="eb84" id="eb84" class="graf graf--li graf-after--li">Detect Age</li><li name="979c" id="979c" class="graf graf--li graf-after--li">Display output</li></ol><p name="3330" id="3330" class="graf graf--p graf-after--li">After you have downloaded the code, you can run it using the sample image provided or using the webcam.</p><pre name="5ba9" id="5ba9" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0">python gad.py <strong class="markup--strong markup--pre-strong">--image</strong>sample1.jpg</code></pre><p name="db7f" id="db7f" class="graf graf--p graf-after--pre">Let us have a look at the code for gender and age prediction using the DNN module in OpenCV. Please download the code as the code snippets are given below are only for the important parts of the code.</p><h3 name="6e2c" id="6e2c" class="graf graf--h3 graf-after--p">2.1. Detect Face</h3><p name="9c88" id="9c88" class="graf graf--p graf-after--h3">I will use the DNN Face Detector for face detection. The model is only 2.7MB and is pretty fast even on the CPU. More details about the face detector can be found in our blog on <a href="https://www.learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/" data-href="https://www.learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/" class="markup--anchor markup--p-anchor" rel="noreferrer noopener noopener" target="_blank">Face Detection</a>. The face detection is done using the function getFaceBox as shown below.</p><pre name="5516" id="5516" class="graf graf--pre graf-after--p">def highlightFace(net, frame, conf_threshold=0.7):<br>frameOpencvDnn=frame.copy()<br>frameHeight=frameOpencvDnn.shape[0]<br>frameWidth=frameOpencvDnn.shape[1]<br>blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)<br>net.setInput(blob)<br>detections=net.forward()<br>faceBoxes=[]</pre><pre name="4523" id="4523" class="graf graf--pre graf-after--pre">for i in range(detections.shape[2]):<br>confidence=detections[0,0,i,2]<br>if confidence&gt;conf_threshold:<br>x1=int(detections[0,0,i,3]*frameWidth)<br>y1=int(detections[0,0,i,4]*frameHeight)<br>x2=int(detections[0,0,i,5]*frameWidth)<br>y2=int(detections[0,0,i,6]*frameHeight)<br>faceBoxes.append([x1,y1,x2,y2])<br>cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)</pre><pre name="ae83" id="ae83" class="graf graf--pre graf-after--pre">return frameOpencvDnn,faceBoxes</pre><h3 name="385d" id="385d" class="graf graf--h3 graf-after--pre">2.2. Predict Gender</h3><p name="8951" id="8951" class="graf graf--p graf-after--h3">I will load the gender network into memory and pass the detected face through the network. The forward pass gives the probabilities or confidence of the two classes. I take the max of the two outputs and use it as the final gender prediction.</p><pre name="5630" id="5630" class="graf graf--pre graf-after--p">genderProto=&quot;gender_deploy.prototxt&quot;<br>genderModel=&quot;gender_net.caffemodel&quot;<br>genderList=[&#39;Male&#39;,&#39;Female&#39;]<br>MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)</pre><pre name="eb86" id="eb86" class="graf graf--pre graf-after--pre">genderNet=cv2.dnn.readNet(genderModel,genderProto)<br>blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)</pre><pre name="8b82" id="8b82" class="graf graf--pre graf-after--pre">genderNet.setInput(blob)<br>genderPreds=genderNet.forward()<br>gender=genderList[genderPreds[0].argmax()]<br>print(f&#39;Gender: {gender}&#39;)</pre><h3 name="00de" id="00de" class="graf graf--h3 graf-after--pre">2.3. Predict Age</h3><p name="f360" id="f360" class="graf graf--p graf-after--h3">I load the age network and use the forward pass to get the output. Since the network architecture is similar to the Gender Network, we can take the max out of all the outputs to get the predicted age group.</p><pre name="f89a" id="f89a" class="graf graf--pre graf-after--p">ageProto=&quot;age_deploy.prototxt&quot;<br>ageModel=&quot;age_net.caffemodel&quot;<br>MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)<br>ageList=[&#39;(0-2)&#39;, &#39;(4-6)&#39;, &#39;(8-12)&#39;, &#39;(15-20)&#39;, &#39;(25-32)&#39;, &#39;(38-43)&#39;, &#39;(48-53)&#39;, &#39;(60-100)&#39;]<br>ageNet=cv2.dnn.readNet(ageModel,ageProto)</pre><pre name="e0d2" id="e0d2" class="graf graf--pre graf-after--pre">blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)<br>ageNet.setInput(blob)<br>agePreds=ageNet.forward()<br>age=ageList[agePreds[0].argmax()]<br>print(f&#39;Age: {age[1:-1]} years&#39;)</pre><h3 name="a222" id="a222" class="graf graf--h3 graf-after--pre">2.4. Display Output</h3><p name="706a" id="706a" class="graf graf--p graf-after--h3">I will display the output of the network on the input images and show them using the imshow function.</p><pre name="7fed" id="7fed" class="graf graf--pre graf-after--p">cv2.putText(resultImg, f&#39;{gender}, {age}&#39;, (faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)</pre><pre name="2434" id="2434" class="graf graf--pre graf-after--pre">cv2.imshow(&quot;Detecting age and gender&quot;, resultImg)</pre><p name="85f6" id="85f6" class="graf graf--p graf-after--pre">If you have video and then we need to identify faces through it.</p><pre name="f6b0" id="f6b0" class="graf graf--pre graf-after--p">video=cv2.VideoCapture(args.image if args.image else 0)<br>padding=20<br>while cv2.waitKey(1)&lt;0:<br>hasFrame,frame=video.read()<br>if not hasFrame:<br>cv2.waitKey()<br>break</pre><pre name="35ae" id="35ae" class="graf graf--pre graf-after--pre">resultImg,faceBoxes=highlightFace(faceNet,frame)</pre><pre name="1001" id="1001" class="graf graf--pre graf-after--pre">if not faceBoxes:<br>print(&quot;No face detected&quot;)</pre><pre name="58da" id="58da" class="graf graf--pre graf-after--pre">for faceBox in faceBoxes:<br>face=frame[max(0,faceBox[1]-padding):<br>min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding)<br>:min(faceBox[2]+padding, frame.shape[1]-1)]</pre><h3 name="4977" id="4977" class="graf graf--h3 graf-after--pre">Python Project Examples for Gender and Age Detection</h3><p name="8bbb" id="8bbb" class="graf graf--p graf-after--h3">Let’s try this gender and age classifier out on some of our own images now.</p><p name="7e66" id="7e66" class="graf graf--p graf-after--p">I’ll get to the command prompt, run our script with the image option, and specify an image to classify:</p><p name="9338" id="9338" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Example 1</strong></p><p name="5bb9" id="5bb9" class="graf graf--p graf-after--p">input:</p><figure name="9a38" id="9a38" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 451px; max-height: 128px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.4%;"></div><img class="graf-image" data-image-id="1*VWImxaa1N4Vi_a_3L38lAA.png" data-width="451" data-height="128" src="https://cdn-images-1.medium.com/max/800/1*VWImxaa1N4Vi_a_3L38lAA.png"></div></figure><p name="219a" id="219a" class="graf graf--p graf-after--figure">output:</p><figure name="7c2c" id="7c2c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 198px; max-height: 300px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 151.5%;"></div><img class="graf-image" data-image-id="1*C049H5mWRyG4mgVJigQtnA.png" data-width="198" data-height="300" src="https://cdn-images-1.medium.com/max/800/1*C049H5mWRyG4mgVJigQtnA.png"></div></figure><p name="7d87" id="7d87" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Example 2</strong></p><p name="51cc" id="51cc" class="graf graf--p graf-after--p">input:</p><figure name="97c0" id="97c0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 363px; max-height: 65px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.9%;"></div><img class="graf-image" data-image-id="1*WNTcZzLfZpxJ48KwcgewaA.png" data-width="363" data-height="65" src="https://cdn-images-1.medium.com/max/800/1*WNTcZzLfZpxJ48KwcgewaA.png"></div></figure><p name="e2ff" id="e2ff" class="graf graf--p graf-after--figure">output:</p><figure name="9b79" id="9b79" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 520px; max-height: 366px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 70.39999999999999%;"></div><img class="graf-image" data-image-id="1*yzlXlcCLnmPyJ8n38nd5ew.png" data-width="520" data-height="366" src="https://cdn-images-1.medium.com/max/800/1*yzlXlcCLnmPyJ8n38nd5ew.png"></div></figure><p name="e7c5" id="e7c5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Example 3</strong></p><p name="1c67" id="1c67" class="graf graf--p graf-after--p">input:</p><figure name="9471" id="9471" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 357px; max-height: 63px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.599999999999998%;"></div><img class="graf-image" data-image-id="1*S_O4pFJxccgPYKR7Pc8I-w.png" data-width="357" data-height="63" src="https://cdn-images-1.medium.com/max/800/1*S_O4pFJxccgPYKR7Pc8I-w.png"></div></figure><p name="6feb" id="6feb" class="graf graf--p graf-after--figure">output:</p><figure name="38bb" id="38bb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 520px; max-height: 419px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.60000000000001%;"></div><img class="graf-image" data-image-id="1*Iuiq0uy5UxkXiOkkaLWMDQ.png" data-width="520" data-height="419" src="https://cdn-images-1.medium.com/max/800/1*Iuiq0uy5UxkXiOkkaLWMDQ.png"></div></figure><p name="7c0a" id="7c0a" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Example 4</strong></p><p name="d94f" id="d94f" class="graf graf--p graf-after--p">input:</p><figure name="144b" id="144b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 369px; max-height: 63px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 17.1%;"></div><img class="graf-image" data-image-id="1*AHIC-h55HqYmu3VpkPiLxA.png" data-width="369" data-height="63" src="https://cdn-images-1.medium.com/max/800/1*AHIC-h55HqYmu3VpkPiLxA.png"></div></figure><p name="8dd0" id="8dd0" class="graf graf--p graf-after--figure">output:</p><figure name="fa90" id="fa90" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 520px; max-height: 363px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.8%;"></div><img class="graf-image" data-image-id="1*4zTobK0vKV0HxWqDEYb3vw.png" data-width="520" data-height="363" src="https://cdn-images-1.medium.com/max/800/1*4zTobK0vKV0HxWqDEYb3vw.png"></div></figure><p name="2a24" id="2a24" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Example 5</strong></p><p name="f3aa" id="f3aa" class="graf graf--p graf-after--p">input:</p><figure name="1d6a" id="1d6a" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 375px; max-height: 73px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.5%;"></div><img class="graf-image" data-image-id="1*42DSmTodA1kdI_fh6ocWOQ.png" data-width="375" data-height="73" src="https://cdn-images-1.medium.com/max/800/1*42DSmTodA1kdI_fh6ocWOQ.png"></div></figure><p name="8554" id="8554" class="graf graf--p graf-after--figure">output:</p><figure name="25a8" id="25a8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 520px; max-height: 341px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 65.60000000000001%;"></div><img class="graf-image" data-image-id="1*LPsuJP-MZkRy-mjUiIWx_A.png" data-width="520" data-height="341" src="https://cdn-images-1.medium.com/max/800/1*LPsuJP-MZkRy-mjUiIWx_A.png"></div></figure><p name="e42f" id="e42f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Example 6</strong></p><p name="f022" id="f022" class="graf graf--p graf-after--p">input:</p><figure name="8106" id="8106" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 371px; max-height: 62px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.7%;"></div><img class="graf-image" data-image-id="1*cytkL82GL5qJzlVBpPwkLQ.png" data-width="371" data-height="62" src="https://cdn-images-1.medium.com/max/800/1*cytkL82GL5qJzlVBpPwkLQ.png"></div></figure><p name="4d6f" id="4d6f" class="graf graf--p graf-after--figure">output:</p><figure name="6581" id="6581" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 520px; max-height: 296px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.89999999999999%;"></div><img class="graf-image" data-image-id="1*lVl3I-ArO3LWGAs6ruZtPQ.png" data-width="520" data-height="296" src="https://cdn-images-1.medium.com/max/800/1*lVl3I-ArO3LWGAs6ruZtPQ.png"></div></figure><p name="7591" id="7591" class="graf graf--p graf-after--figure">We saw above that the network is able to predict both Gender and Age to a high level of accuracy. Next, we wanted to do something interesting with this model. Many actors have portrayed the role of the opposite gender in movies.</p><blockquote name="a0dd" id="a0dd" class="graf graf--blockquote graf-after--p">We want to check what AI says about their looks in these roles and whether they are able to fool the AI.</blockquote><p name="ba85" id="ba85" class="graf graf--p graf-after--blockquote">I used images from <a href="https://brightside.me/wonder-films/14-actors-who-masterfully-played-opposite-genders-345460/" data-href="https://brightside.me/wonder-films/14-actors-who-masterfully-played-opposite-genders-345460/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this article</a> which shows their actual photographs along with those from the movies in which they changed their gender. Let’s have a look.</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="53f3" id="53f3" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 55.648%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.699999999999996%;"></div><img class="graf-image" data-image-id="1*POKEuGeLzTyoo36RirijPA.jpeg" data-width="650" data-height="388" src="https://cdn-images-1.medium.com/max/800/1*POKEuGeLzTyoo36RirijPA.jpeg"></div></figure><figure name="8320" id="8320" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 44.352%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.9%;"></div><img class="graf-image" data-image-id="1*rFOZl7xz2tExq34PaUQQIw.jpeg" data-width="650" data-height="487" src="https://cdn-images-1.medium.com/max/600/1*rFOZl7xz2tExq34PaUQQIw.jpeg"></div></figure></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="2d3e" id="2d3e" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 56.084%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.1%;"></div><img class="graf-image" data-image-id="1*0eMzH7_CBO0gqPNkR7zvow.jpeg" data-width="650" data-height="358" src="https://cdn-images-1.medium.com/max/800/1*0eMzH7_CBO0gqPNkR7zvow.jpeg"></div></figure><figure name="f899" id="f899" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 43.916%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 70.3%;"></div><img class="graf-image" data-image-id="1*uHQwx_Ml66usJRkU_hoq6w.jpeg" data-width="650" data-height="457" src="https://cdn-images-1.medium.com/max/600/1*uHQwx_Ml66usJRkU_hoq6w.jpeg"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><h3 name="9606" id="9606" class="graf graf--h3 graf-after--figure">Observations</h3><p name="9881" id="9881" class="graf graf--p graf-after--h3">Even though the gender prediction network performed well, the age prediction network fell short of our expectations. I tried to find the answer in the paper and found the following confusion matrix for the age prediction model.</p><figure name="c26e" id="c26e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 346px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 49.4%;"></div><img class="graf-image" data-image-id="1*CvbMoly28k2h6g4z9djiqg.jpeg" data-width="700" data-height="346" src="https://cdn-images-1.medium.com/max/800/1*CvbMoly28k2h6g4z9djiqg.jpeg"></div></figure><p name="d0e3" id="d0e3" class="graf graf--p graf-after--figure">The following observations can be made from the above table :</p><ul class="postList"><li name="278c" id="278c" class="graf graf--li graf-after--p">The age groups 0–2, 4–6, 8–13, and 25–32 are predicted with relatively high accuracy. ( see the diagonal elements )</li><li name="c139" id="c139" class="graf graf--li graf-after--li">The output is heavily biased towards the age group 25–32 ( see the row belonging to the age group 25–32 ). This means that it is very easy for the network to get confused between the ages of 15 to 43. So, even if the actual age is between 15–20 or 38–43, there is a high chance that the predicted age will be 25–32. This is also evident from the Results section.</li></ul><p name="4e1e" id="4e1e" class="graf graf--p graf-after--li">Apart from this, I observed that the accuracy of the models improved if I use padding around the detected face. This may be due to the fact that the input while training was standard face images and not closely cropped faces that we get after face detection.</p><p name="7550" id="7550" class="graf graf--p graf-after--p">I also analyzed the use of face alignment before making predictions and found that the predictions improved for some examples but at the same time, it became worse for some. It may be a good idea to use alignment if you are mostly working with non-frontal faces.</p><p name="83de" id="83de" class="graf graf--p graf-after--p">Thank You For Reading!</p><p name="a14b" id="a14b" class="graf graf--p graf-after--p">We’ll get to the command prompt, run our script with the image option and specify an image to classify:</p><p name="c100" id="c100" class="graf graf--p graf-after--p graf--trailing">Python Project Example 1</p></div></div></section>
</section>
</article></body></html>